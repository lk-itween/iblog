{
  
    
        "post0": {
            "title": "Fastpages汉化版使用说明",
            "content": "1 登录自己的github账户，访问：https://github.com/EasonQYS/fastpagesJupyter ，找到下面这个蓝色的字并点击， . . 在自己的账户下生成一个仓库（仓库名不能使用），用于运行网页。 . 2 稍等片刻（约30秒），你的github邮箱会收到一封邮件，指导你接下来的步骤。由于邮件是英文版，请参考README文档。 . 操作过程截图： . . . . 点击Add key后需要输入github密码。 . 3查看Actions，绿色对勾表示初始化完成。（这一步没有意义） . . 4在https://github.com/EasonQYS/fastpagesJupyter/settings settings页面下滑找到 . . 选择master branch，等待https://github.com/EasonQYS/fastpagesJupyter/actions 显示成功。 . 5如果运行正常，会生成一个新的branch，名为gh-pages。需要将网页改至该路径。（若干没有，可以先进行后续操作。稍后在回来做这一步） . . 至此，网站已经可以显示了。 . 6 修改_config.yml . 为了让页面显示更美观，以及修改超链接为正确内容，我们可能要做一下修改。 . 需要修改的部分均已给出中文标注。 . 相关修改参考见截图 . . 若要修改网页标题，则修改 . . 若要修改网页内容，则修改index.html文件。 . 现在网页正常显示了。 . 7 如果没有做第5步，现在做。做完后网站正常显示。 . . 8 上传Jupyter文件 . 所有ipynb文件都在_notebooks目录下，其中的readme要求命名格式为YYYY-MM-DD-*.ipynb，注意，很容易忘记最后一个破折号。 . 但是也有要求，第一个单元格内要以Markdown形式写： . . . 写完上传至_notebooks目录。如果是本地克隆文件夹，那么记得同步至github云端。 . 上传之后等待一段时间，即可在博客上看到这篇文章。 . 9 类似的，Markdown文件在_posts目录下，也能自动转换。 . 同样，有格式要求。（支持中文名） . 当然，少一点也行，这是一个例子： . 在_posts目录下创建正确命名的md文档，写入后保存。若在本地写好，记得同步（克隆）至云端。 . -– . layout: post . title: “Welcome to Jekyll!” . -– . # Welcome . **Hello world**, this is my first Jekyll blog post. . I hope you like it! . 10 此外，_docs还能转换docx文件，方法类似。 . 将文档上传至/_docs目录下即可。 . 11 删帖，删除上述文件即可。 . 遇到的坑： . 1、md文档可以转博客，但是rst文档好像不行。解决方案，将rst文件重命名时后缀改为md。 . 2、若上传多个文档，其中一个报错，同批所有文档都不显示。故每次上传一个文档为佳，利于排查。每个文档生成网页的时间大约在2分钟左右。需要等待至Action成绿色√才表示完成，可以查看网页。 . . 3、可加载网页图片，但不能加载本地图片（文档中自带的图片）。例如下图方式添加的图片不能显示。 . .",
            "url": "/2020/04/14/fastpages%E6%B1%89%E5%8C%96%E7%89%88%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.html",
            "relUrl": "/2020/04/14/fastpages%E6%B1%89%E5%8C%96%E7%89%88%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.html",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "欢迎（ms Word）",
            "content": "欢迎你 . 欢迎你 . 欢迎你 . 欢迎你 . .",
            "url": "/2020/04/13/%E6%AC%A2%E8%BF%8E-MS-Word.html",
            "relUrl": "/2020/04/13/%E6%AC%A2%E8%BF%8E-MS-Word.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "欢迎使用（Jupyter）",
            "content": "&#20320;&#22909; . print(&quot;你好！欢迎使用！&quot;) . 你好！欢迎使用！ .",
            "url": "/jupyter/%E4%B8%AD%E6%96%87/%E6%AC%A2%E8%BF%8E/2020/04/13/%E6%AC%A2%E8%BF%8E%E4%BD%BF%E7%94%A8-Jupyter.html",
            "relUrl": "/jupyter/%E4%B8%AD%E6%96%87/%E6%AC%A2%E8%BF%8E/2020/04/13/%E6%AC%A2%E8%BF%8E%E4%BD%BF%E7%94%A8-Jupyter.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "淘宝用户行为分析",
            "content": "&#19968;&#12289;&#39033;&#30446;&#32972;&#26223; . 通过天池提供的淘宝app平台数据集，通过行业的指标对淘宝用户行为进行分析，探索淘宝用户行为模式。 . 数据来源: https://tianchi.aliyun.com/dataset/dataDetail?dataId=46&amp;userId=1 . &#20108;&#12289;&#20869;&#23481;&#25506;&#32034; . 日PV有多少? | 日UV有多少? | 付费率情况如何? | 复购率表现怎样? | 漏斗流失情况如何? | 用户价值情况? | &#19977;&#12289;&#25968;&#25454;&#23548;&#20837; . 数据集有104万条左右数据，统计年份为2014年11月18日至2014年12月18日，数据均已脱敏。 . 字段描述: . user_id: 用户ID item_id: 商品ID behavior_type: 用户行为类型(点击、收藏，加购，支付,分别用数字1、2、3、4表示) user_geohash: 地理位置 item_category: 商品的类别ID time: 行为时间 . # 导入必要模块 import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . # 解决可视化中的中文显示问题 plt.rcParams[&#39;font.family&#39;] = &#39;sans-serif&#39; plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;STSong&#39;] plt.rcParams[&#39;axes.unicode_minus&#39;] = False # 负号显示 . path = &#39;D:/shujufenxi/xls_shuju/淘宝App用户使用行为数据集/&#39; . data = pd.read_csv(path + &#39;tianchi_mobile_recommend_train_user.csv&#39;) data.head() . user_id item_id behavior_type user_geohash item_category time . 0 98047837 | 232431562 | 1 | NaN | 4245 | 2014-12-06 02 | . 1 97726136 | 383583590 | 1 | NaN | 5894 | 2014-12-09 20 | . 2 98607707 | 64749712 | 1 | NaN | 2883 | 2014-12-18 11 | . 3 98662432 | 320593836 | 1 | 96nn52n | 6562 | 2014-12-06 10 | . 4 98145908 | 290208520 | 1 | NaN | 13926 | 2014-12-16 21 | . # 查看数据类型 data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 12256906 entries, 0 to 12256905 Data columns (total 6 columns): user_id int64 item_id int64 behavior_type int64 user_geohash object item_category int64 time object dtypes: int64(4), object(2) memory usage: 561.1+ MB . # 缺失值处理 data.isnull().sum() . user_id 0 item_id 0 behavior_type 0 user_geohash 8334824 item_category 0 time 0 dtype: int64 . 虽然user_geohash存在缺失值，但通过经验判断，App会将地理信息做加密处理，所以不能删除缺失值，暂不对数据集做处理 . # 时间字段处理 data[&#39;date&#39;] = data[&#39;time&#39;].apply(lambda x: x.split(&#39; &#39;)[0]) data[&#39;hour&#39;] = data[&#39;time&#39;].apply(lambda x: x.split(&#39; &#39;)[1]) data.head() . user_id item_id behavior_type user_geohash item_category time date hour . 0 98047837 | 232431562 | 1 | NaN | 4245 | 2014-12-06 02 | 2014-12-06 | 02 | . 1 97726136 | 383583590 | 1 | NaN | 5894 | 2014-12-09 20 | 2014-12-09 | 20 | . 2 98607707 | 64749712 | 1 | NaN | 2883 | 2014-12-18 11 | 2014-12-18 | 11 | . 3 98662432 | 320593836 | 1 | 96nn52n | 6562 | 2014-12-06 10 | 2014-12-06 | 10 | . 4 98145908 | 290208520 | 1 | NaN | 13926 | 2014-12-16 21 | 2014-12-16 | 21 | . data[&#39;time&#39;] = pd.to_datetime(data[&#39;time&#39;]) data[&#39;date&#39;] = pd.to_datetime(data[&#39;date&#39;]) data[&#39;hour&#39;] = data[&#39;hour&#39;].astype(&#39;int64&#39;) . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 12256906 entries, 0 to 12256905 Data columns (total 8 columns): user_id int64 item_id int64 behavior_type int64 user_geohash object item_category int64 time datetime64[ns] date datetime64[ns] hour int64 dtypes: datetime64[ns](2), int64(5), object(1) memory usage: 748.1+ MB . # 异常值处理 data.describe() . user_id item_id behavior_type item_category hour . count 1.225691e+07 | 1.225691e+07 | 1.225691e+07 | 1.225691e+07 | 1.225691e+07 | . mean 7.170732e+07 | 2.023084e+08 | 1.105271e+00 | 6.846162e+03 | 1.481799e+01 | . std 4.122920e+07 | 1.167397e+08 | 4.572662e-01 | 3.809922e+03 | 6.474778e+00 | . min 4.913000e+03 | 6.400000e+01 | 1.000000e+00 | 2.000000e+00 | 0.000000e+00 | . 25% 3.584965e+07 | 1.014130e+08 | 1.000000e+00 | 3.721000e+03 | 1.000000e+01 | . 50% 7.292804e+07 | 2.021359e+08 | 1.000000e+00 | 6.209000e+03 | 1.600000e+01 | . 75% 1.073774e+08 | 3.035405e+08 | 1.000000e+00 | 1.029000e+04 | 2.000000e+01 | . max 1.424559e+08 | 4.045625e+08 | 4.000000e+00 | 1.408000e+04 | 2.300000e+01 | . 观察数据集的总体描述没有异常值存在 . &#22235;&#12289;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512; . 1. PV&#21644;UV&#20998;&#26512; . # PV（访问量/Page View）: 指页面的浏览量或者点击量，页面没刷新一次就计算一次 pv_daily = data.groupby(&#39;date&#39;)[&#39;user_id&#39;].count().reset_index().rename(columns={&#39;user_id&#39;:&#39;pv&#39;}) . # UV(独立访客/Unique Vistior): 访问网站的一台电脑客户端为一个访客 uv_daily = data.groupby(&#39;date&#39;)[&#39;user_id&#39;].apply(lambda x: x.drop_duplicates().count()).reset_index().rename(columns={&#39;user_id&#39;:&#39;uv&#39;}) . from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() . fig = plt.figure(figsize=(18,8)) ax1 = fig.add_subplot(1,1,1) ax1.plot(pv_daily[&#39;date&#39;], pv_daily[&#39;pv&#39;], color=&#39;red&#39;, label=&#39;pv_daily&#39;) ax1.grid(True, linestyle=&quot;--&quot;, alpha=0.5) ax2 = ax1.twinx() ax2.plot(uv_daily[&#39;date&#39;], uv_daily[&#39;uv&#39;], color=&#39;blue&#39;, label=&#39;uv_daily&#39;) # 图例显示 handles1, labels1 = ax1.get_legend_handles_labels() handles2, labels2 = ax2.get_legend_handles_labels() plt.legend(handles1+handles2, labels1+labels2, loc=&#39;best&#39;, fontsize=16) plt.show() . 如今电商节日增多，像&quot;6·18&quot;,&quot;双11&quot;,&quot;双12&quot;都是促进用户消费的日期，可见在截取的30天的数据里，双12期间，PV和UV访问量达到峰值，而且，PV与UV的差值较大，但趋势一致，可以认为人均日访问量不变。通过UV可以分析出，数据集中的总人数大约8000人，双十二期间淘宝app用户的日活跃有45%左右的浮动。 . # 按小时量分析 PV和UV pv_hour = data.groupby(&#39;hour&#39;)[&#39;user_id&#39;].count().reset_index().rename(columns={&#39;user_id&#39;:&#39;pv&#39;}) uv_hour = data.groupby(&#39;hour&#39;)[&#39;user_id&#39;].apply(lambda x: x.drop_duplicates().count()).reset_index().rename(columns={&#39;user_id&#39;:&#39;uv&#39;}) . fig = plt.figure(figsize=(12,6)) ax1 = fig.add_subplot(1,1,1) ax1.plot(pv_hour[&#39;hour&#39;], pv_hour[&#39;pv&#39;], color=&#39;red&#39;, label=&#39;pv_hour&#39;) ax1.set_xticks(np.arange(0,24,1)) ax1.grid(True, linestyle=&quot;--&quot;, alpha=0.5) ax2 = ax1.twinx() ax2.plot(uv_hour[&#39;hour&#39;], uv_hour[&#39;uv&#39;], color=&#39;blue&#39;, label=&#39;uv_hour&#39;) # 图例显示 handles1, labels1 = ax1.get_legend_handles_labels() handles2, labels2 = ax2.get_legend_handles_labels() plt.legend(handles1+handles2, labels1+labels2, loc=&#39;best&#39;, fontsize=14) plt.show() . 在夜里21点-次日5点间，PV和UV波动一致，成下降趋势，访问量从峰值下降到谷底，这和生活作息相关，但在白天，UV值在早上8点就达到峰值，PV处于正常水平，在18点开始处于上升状态，至21点到达峰值，说明，晚上18点以后是淘宝用户的访问app的活跃时间段，与正常的作息时间一致。 . # 分析 各用户行为类型的 PV和UV pv_detail = data.groupby([&#39;behavior_type&#39;, &#39;date&#39;])[&#39;user_id&#39;].count().reset_index().rename(columns={&#39;user_id&#39;:&#39;total_pv&#39;}) . # 用户行为类型(点击、收藏，加购，支付,分别用数字1、2、3、4表示) # 点击行为往往比其他行为更多，单独画图 fig = plt.figure(figsize=(12,6)) ax1 = fig.add_subplot(2,1,1) sns.pointplot(x=&#39;date&#39;, y=&#39;total_pv&#39;, hue=&#39;behavior_type&#39;, data=pv_detail[pv_detail.behavior_type == 1], ax=ax1) x_tick = list(range(0,len(pv_detail.date.unique()),5)) x_label = [pv_detail.date[i].strftime(&#39;%Y-%m-%d&#39;) for i in x_tick] ax1.set_xticks(x_tick) ax1.set_xticklabels(x_label,rotation=0) ax2 = fig.add_subplot(2,1,2) sns.pointplot(x=&#39;date&#39;, y=&#39;total_pv&#39;, hue=&#39;behavior_type&#39;, data=pv_detail[pv_detail.behavior_type != 1], ax=ax2) ax2.set_xticks(x_tick) ax2.set_xticklabels(x_label,rotation=0) plt.show() . 在这张图中，尽管在双12期间，点击、收藏、加购的用户行为增多，但收藏行为还是和往常一样。这可能因为，双12活动促销，商品品类增多，致使用户查看商品的行为增多，也会产生大量的购买欲望，不再将商品收藏，直接买买买。 . pv_detail_hour = data.groupby([&#39;behavior_type&#39;, &#39;hour&#39;])[&#39;user_id&#39;].count().reset_index().rename(columns={&#39;user_id&#39;:&#39;total_pv_hour&#39;}) . # 用户行为类型(点击、收藏，加购，支付,分别用数字1、2、3、4表示) # 点击行为往往比其他行为更多，单独画图 fig = plt.figure(figsize=(12,6)) ax1 = fig.add_subplot(2,1,1) sns.pointplot(x=&#39;hour&#39;, y=&#39;total_pv_hour&#39;, hue=&#39;behavior_type&#39;, data=pv_detail_hour[pv_detail_hour.behavior_type == 1], ax=ax1) ax2 = fig.add_subplot(2,1,2) sns.pointplot(x=&#39;hour&#39;, y=&#39;total_pv_hour&#39;, hue=&#39;behavior_type&#39;, data=pv_detail_hour[pv_detail_hour.behavior_type != 1], ax=ax2) plt.show() . 可以看到点击行为相较于其他用户行为，pv访问量较高，而且支付量是较低的，符合用户行为漏斗流失模型，收藏量对比加购行为较低，主要原因App的加购按钮比收藏更为显眼，而且购物车有个单独的显示页面，用户可以更方便的了解购物车页面商品的信息，四种行为在整体趋势上有一致性，在晚上，无论暗中用户行为，pv访问量都是最高的。 . 2. &#29992;&#25143;&#28040;&#36153;&#34892;&#20026;&#20998;&#26512; . # 1). 用户购买次数情况分析 user_buy = data[data.behavior_type==4].groupby(&#39;user_id&#39;)[&#39;behavior_type&#39;].count() . sns.distplot(user_buy, bins=100, kde=False) plt.title(&#39;user_buy of daily&#39;) print(&#39;用户购买次数中位数: &#39;,user_buy.median()) plt.show() . 用户购买次数中位数: 8.0 . 淘宝App用户每月消费次数普遍在10次以内，因此本次分析重点关注次数在10次以上的消费者用户群体，为什么会产生高频次购买行为。 . # 日ARPPU （指从每位付费用户身上获得的收入，反映的是每个付费用户的平均付费额度） # ARPPU = 总收入/活跃用户付费数量 # 付费人均消费次数 = 消费总次数/付费总人数 daily_ARPPU = data[data.behavior_type==4].groupby([&#39;date&#39;,&#39;user_id&#39;])[&#39;behavior_type&#39;].count().reset_index().rename(columns={&#39;behavior_type&#39;:&#39;total&#39;}) daily_ARPPU = daily_ARPPU.groupby(&#39;date&#39;).apply(lambda x: x.total.sum()/x.total.count()) plt.figure(figsize=(18,8)) plt.plot(daily_ARPPU) plt.title(&#39;daily_ARPPU&#39;) plt.show() . 在这30天的数据中，付费用户平均购买次数大约为2次，双十二达到人均购买4次。 . # 日ARPU：平均每用户收入，可通过总收入/总人数计算得出，用来衡量盈利能力和发展活力 # 活跃用户数平均消费次数=消费总次数/活跃用户人数 data[&#39;operation&#39;] = 1 # 设定操作值，便于计数 daily_ARPU = data.groupby([&#39;date&#39;,&#39;user_id&#39;,&#39;behavior_type&#39;])[&#39;operation&#39;].count().reset_index().rename(columns={&#39;operation&#39;:&#39;total&#39;}) daily_ARPU = daily_ARPU.groupby(&#39;date&#39;).apply(lambda x:x[x.behavior_type==4].total.sum()/len(x.user_id.unique())) plt.figure(figsize=(18,8)) plt.plot(daily_ARPU) plt.title(&#39;daily_ARPU&#39;) plt.show() . 双十二促销活动中，人均消费次数达2次 . # 同时间段用户消费次数分布 user_count_of_buy = data[data.behavior_type==4].groupby([&#39;user_id&#39;,&#39;date&#39;,&#39;hour&#39;])[&#39;operation&#39;].sum().rename(&#39;buy_count&#39;) plt.figure() sns.distplot(user_count_of_buy) print(&#39;用户消费次数众数: &#39;, user_count_of_buy.mode()[0]) . 用户消费次数众数: 1 . 3.&#22797;&#36141;&#24773;&#20917;&#20998;&#26512; . 复购行为: 两天以上有购买行为，一天内购买多次算一次 | 复购率: 有复购行为的用户数 / 购买用户总人数 | . date_rebuy = data[data[&#39;behavior_type&#39;]==4].groupby(&#39;user_id&#39;)[&#39;date&#39;].apply(lambda x:len(x.unique())) print(&#39;复购率: &#39;, round(date_rebuy[date_rebuy&gt;=2].count()/date_rebuy.count(),4)) . 复购率: 0.8717 . # 所有复购时间间隔消费次数分布 date_buy_day = data[data.behavior_type==4].groupby([&#39;user_id&#39;,&#39;date&#39;]).operation.count().reset_index() date_buy_day = date_buy_day.groupby(&#39;user_id&#39;)[&#39;date&#39;].apply(lambda x:x.sort_values().diff(1).dropna()).reset_index() date_buy_day = date_buy_day[&#39;date&#39;].apply(lambda x:x.days) . plt.figure(figsize=(12,6)) date_buy_day.value_counts().plot(kind=&#39;bar&#39;) plt.xticks(rotation=0) plt.title(&#39;time_gap&#39;) plt.xlabel(&#39;gap_day&#39;) plt.ylabel(&#39;gap_count&#39;) plt.show() . print(&#39;多数人复购率: &#39;,round(date_rebuy[date_rebuy&gt;=2].value_counts().head(3).sum()/date_rebuy.count(),2)) . 多数人复购率: 0.36 . 多数用户复购率为0.36，消费次数随着消费时间间隔的增加而不断下降，间隔天数达10天后，淘宝App用户很少进行复购行为，需要加强对淘宝复购用户10天之内行为监测，以提高淘宝App的复购率。 . 4.&#28431;&#26007;&#27969;&#22833;&#20998;&#26512; . 漏斗分析可以科学反映用户行为状态，从流程起点到终点的转化率情况的重要分析模型，是一个流程式数据分析。 . user_count = data.groupby(&#39;behavior_type&#39;).count() user_count . user_id item_id user_geohash item_category time date hour operation . behavior_type . 1 11550581 | 11550581 | 3704666 | 11550581 | 11550581 | 11550581 | 11550581 | 11550581 | . 2 242556 | 242556 | 74365 | 242556 | 242556 | 242556 | 242556 | 242556 | . 3 343564 | 343564 | 104443 | 343564 | 343564 | 343564 | 343564 | 343564 | . 4 120205 | 120205 | 38608 | 120205 | 120205 | 120205 | 120205 | 120205 | . # 不同用户行为类别的转化率 conversion_rate = data[data.behavior_type!=2].groupby(&#39;behavior_type&#39;)[&#39;operation&#39;].count() . # 漏斗图展示 from pyecharts.charts import Funnel from pyecharts import options as opts . funnel = (Funnel() .add(&#39;用户数&#39;,[list(z) for z in zip([&#39;点击量&#39;,&#39;加入购物车&#39;,&#39;购买量&#39;],conversion_rate.values.tolist())], label_opts=opts.LabelOpts(position=&#39;inside&#39;)) .set_global_opts(title_opts=opts.TitleOpts(title=&#39;淘宝App用户行为漏斗图&#39;)) ) funnel.render(&#39;淘宝App用户行为漏斗图.html&#39;) funnel.render_notebook() . # 用户行为与商品品类的转换率 cate_rate = data[data.behavior_type!=2].groupby([&#39;item_category&#39;,&#39;behavior_type&#39;])[&#39;operation&#39;].count().unstack().rename(columns={1:&#39;点击量&#39;, 3:&#39;加入购物车&#39;, 4:&#39;购买量&#39;}).fillna(0) . cate_rate.sort_values(by=[&#39;点击量&#39;,&#39;购买量&#39;],ascending=False).head(15) . behavior_type 点击量 加入购物车 购买量 . item_category . 1863 371738.0 | 9309.0 | 2000.0 | . 13230 342694.0 | 6012.0 | 841.0 | . 5027 320870.0 | 5564.0 | 858.0 | . 5894 314784.0 | 6615.0 | 958.0 | . 6513 281370.0 | 6651.0 | 1059.0 | . 5399 268639.0 | 5430.0 | 1054.0 | . 11279 177961.0 | 3686.0 | 722.0 | . 2825 155949.0 | 3692.0 | 625.0 | . 5232 135506.0 | 4486.0 | 1611.0 | . 10894 131221.0 | 3894.0 | 865.0 | . 4370 115048.0 | 3424.0 | 775.0 | . 5689 109697.0 | 2861.0 | 563.0 | . 6000 105163.0 | 1266.0 | 301.0 | . 3064 105034.0 | 3918.0 | 806.0 | . 5395 93082.0 | 1719.0 | 162.0 | . ## 用户最常购买的商品 item_user = data.item_id.value_counts().head(10) item_user.plot(kind=&#39;bar&#39;) number = item_user.values.tolist() for x,y in zip(range(10),number): plt.text( x, y, y, ha=&#39;center&#39;, va=&#39;bottom&#39;, fontsize=8) plt.xticks(rotation=45) plt.show() . # 用户最常购买的商品品类 cate_user = data.item_category.value_counts().head(10) cate_user.plot(kind=&#39;bar&#39;) number = cate_user.values.tolist() for x,y in zip(range(10),number): plt.text(x, y, y, ha=&#39;center&#39;, va=&#39;bottom&#39;, fontsize=8) plt.xticks(rotation=45) plt.show() . &#20116;&#12289;&#29992;&#25143;&#20215;&#20540;&#20998;&#23618;&#65288;RFM&#27169;&#22411;&#65289; . 由于数据集中没有消费金额(M)，所以暂且通过最近一次购买时间(R)和消费频率(F)的数据对客户的价值进行分层。 将最后购买时间定为假定统计时间2014年12月20日 . from datetime import datetime . base = datetime(2014,12,20) recent_buy = data[data[&#39;behavior_type&#39;]==4].groupby(&#39;user_id&#39;)[&#39;date&#39;].apply(lambda x: (base-x.sort_values()).iloc[-1].days) recent_buy = recent_buy.reset_index().rename(columns={&#39;date&#39;:&#39;recent&#39;}) recent_buy.head() . user_id recent . 0 4913 | 4 | . 1 6118 | 3 | . 2 7528 | 7 | . 3 7591 | 7 | . 4 12645 | 6 | . freq_buy = data[data[&#39;behavior_type&#39;]==4].groupby(&#39;user_id&#39;)[&#39;date&#39;].count() freq_buy = freq_buy.reset_index().rename(columns={&#39;date&#39;:&#39;freq&#39;}) freq_buy.head() . user_id freq . 0 4913 | 6 | . 1 6118 | 1 | . 2 7528 | 6 | . 3 7591 | 21 | . 4 12645 | 8 | . RFM = recent_buy.merge(freq_buy, how=&#39;inner&#39;, on=&#39;user_id&#39;) RFM.head() . user_id recent freq . 0 4913 | 4 | 6 | . 1 6118 | 3 | 1 | . 2 7528 | 7 | 6 | . 3 7591 | 7 | 21 | . 4 12645 | 6 | 8 | . # RFM 各个业务分组要求不一样，这里暂且以桶分法分组 RFM[&#39;recent_value&#39;] = pd.qcut(RFM.recent, 2, labels=[&#39;1&#39;,&#39;0&#39;]) RFM[&#39;freq_value&#39;] = pd.qcut(RFM.freq, 2, labels=[&#39;0&#39;,&#39;1&#39;]) RFM.head() . user_id recent freq recent_value freq_value . 0 4913 | 4 | 6 | 1 | 0 | . 1 6118 | 3 | 1 | 1 | 0 | . 2 7528 | 7 | 6 | 0 | 0 | . 3 7591 | 7 | 21 | 0 | 1 | . 4 12645 | 6 | 8 | 1 | 0 | . RFM[&#39;RFM&#39;] = RFM[&#39;recent_value&#39;].str.cat(RFM[&#39;freq_value&#39;]) RFM.head() . user_id recent freq recent_value freq_value RFM . 0 4913 | 4 | 6 | 1 | 0 | 10 | . 1 6118 | 3 | 1 | 1 | 0 | 10 | . 2 7528 | 7 | 6 | 0 | 0 | 00 | . 3 7591 | 7 | 21 | 0 | 1 | 01 | . 4 12645 | 6 | 8 | 1 | 0 | 10 | . # 划分用户等级 def level_RFM(x): if x == &#39;11&#39;: return &#39;重要价值客户&#39; elif x == &#39;10&#39;: return &#39;重要发展客户&#39; elif x == &#39;01&#39;: return &#39;重要保持客户&#39; elif x == &#39;00&#39;: return &#39;一般价值客户&#39; RFM[&#39;用户等级&#39;] = RFM[&#39;RFM&#39;].apply(level_RFM) . RFM.head() . user_id recent freq recent_value freq_value RFM 用户等级 . 0 4913 | 4 | 6 | 1 | 0 | 10 | 重要发展客户 | . 1 6118 | 3 | 1 | 1 | 0 | 10 | 重要发展客户 | . 2 7528 | 7 | 6 | 0 | 0 | 00 | 一般价值客户 | . 3 7591 | 7 | 21 | 0 | 1 | 01 | 重要保持客户 | . 4 12645 | 6 | 8 | 1 | 0 | 10 | 重要发展客户 | . # 用户等级占比 plt.figure(figsize=(4,4)) plt.pie(RFM[&#39;用户等级&#39;].value_counts(), labels=RFM[&#39;用户等级&#39;].unique(), autopct=&#39;%1.2f%%&#39;, colors=(&#39;blue&#39;, &#39;green&#39;, &#39;red&#39;, &#39;orange&#39;)) plt.title(&#39;用户等级占比&#39;) plt.show() . 对于重要价值用户，他们是最优质的用户，需要重点关注并保持， 应该提高满意度，增加留存； . 对于重要保持用户，他们最近有购买，但购买频率不高，可以通过活动等提高其购买频率； . 对于重要发展用户，他们虽然最近没有购买，但以往购买频率高，可以做触达，以防止流失； . 对于一般价值用户，他们最近没有购买，以往购买频率也不高，特别容易流失，所以应该赠送优惠券或推送活动信息，唤醒购买意愿。 . &#24635;&#32467; . 结论: . 用户在一天内消费最高的时间段是晚上18点到23点，其中21点达到峰值; | 在双十二促销日里，用户消费水平比其他日期要高; | 用户点击率较高的商品支付量要高; | 加入购物车的商品有较大的几率转换为已支付商品; | 用户最常购买的商品为11292137，商品品类为1863; | 一般价值用户和重要保持用户占比较高。 | 建议: . 增加客服在晚间的工作时间，引导用户成功购买商品; | 优化电商平台的搜索匹配度和推荐策略。增加用户点击率; | 适当增加双十二的活动时长，让用户有足够的时间浏览新上架的商品，增加点击量; | 可以考虑增加用户最常购买的商品品类的库存量; | 根据RFM模型，找到最具价值的核心付费用户群，对其进行重点关注; | 对于重要价值用户，可以针对该用户开展相应的运营活动，提高用户产品使用频率。 |",
            "url": "/jupyter/taobao/data_analysis/2020/04/13/taobaoApp-analysis.html",
            "relUrl": "/jupyter/taobao/data_analysis/2020/04/13/taobaoApp-analysis.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "欢迎使用Jekyll!（Markdown）",
            "content": "Jekyll博客将从这里开始. . 希望你喜欢! .",
            "url": "/2020/04/12/%E4%BD%A0%E5%A5%BD.html",
            "relUrl": "/2020/04/12/%E4%BD%A0%E5%A5%BD.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "GitHub Actions: Providing Data Scientists With New Superpowers",
            "content": "What Superpowers? . Hi, I’m Hamel Husain. I’m a machine learning engineer at GitHub. Recently, GitHub released a new product called GitHub Actions, which has mostly flown under the radar in the machine learning and data science community as just another continuous integration tool. . Recently, I’ve been able to use GitHub Actions to build some very unique tools for Data Scientists, which I want to share with you today. Most importantly, I hope to get you excited about GitHub Actions, and the promise it has for giving you new superpowers as a Data Scientist. Here are two projects I recently built with Actions that show off its potential: . fastpages . fastpages is an automated, open-source blogging platform with enhanced support for Jupyter notebooks. You save your notebooks, markdown, or Word docs into a directory on GitHub, and they automatically become blog posts. Read the announcement below: . Machine Learning Ops . Wouldn’t it be cool if you could invoke a chatbot natively on GitHub to test your machine learning models on the infrastructure of your choice (GPUs), log all the results, and give you a rich report back in a pull request so that everyone could see the results? You can with GitHub Actions! . Consider the below annotated screenshot of this Pull Request: . . A more in-depth explanation about the above project can be viewed in this video: . Using GitHub Actions for machine learning workflows is starting to catch on. Julien Chaumond, CTO of Hugging Face, says: . GitHub Actions are great because they let us do CI on GPUs (as most of our users use the library on GPUs not on CPUs), on our own infra! 1 . Additionally, you can host a GitHub Action for other people so others can use parts of your workflow without having to re-create your steps. I provide examples of this below. . A Gentle Introduction To GitHub Actions . What Are GitHub Actions? . GitHub Actions allow you to run arbitrary code in response to events. Events are activities that happen on GitHub such as: . Opening a pull request | Making an issue comment | Labeling an issue | Creating a new branch | … and many more | . When an event is created, the GitHub Actions context is hydrated with a payload containing metadata for that event. Below is an example of a payload that is received when an issue is created: . { &quot;action&quot;: &quot;created&quot;, &quot;issue&quot;: { &quot;id&quot;: 444500041, &quot;number&quot;: 1, &quot;title&quot;: &quot;Spelling error in the README file&quot;, &quot;user&quot;: { &quot;login&quot;: &quot;Codertocat&quot;, &quot;type&quot;: &quot;User&quot;, }, &quot;labels&quot;: [ { &quot;id&quot;: 1362934389, &quot;node_id&quot;: &quot;MDU6TGFiZWwxMzYyOTM0Mzg5&quot;, &quot;name&quot;: &quot;bug&quot;, } ], &quot;body&quot;: &quot;It looks like you accidently spelled &#39;commit&#39; with two &#39;t&#39;s.&quot; } . This functionality allows you to respond to various events on GitHub in an automated way. In addition to this payload, GitHub Actions also provide a plethora of variables and environment variables that afford easy to access metadata such as the username and the owner of the repo. Additionally, other people can package useful functionality into an Action that other people can inherit. For example, consider the below Action that helps you publish python packages to PyPi: . The Usage section describes how this Action can be used: . - name: Publish a Python distribution to PyPI uses: pypa/gh-action-pypi-publish@master with: user: __token__ password: ${{ secrets.pypi_password }} . This Action expects two inputs: user and a password. You will notice that the password is referencing a variable called secrets, which is a variable that contains an encrypted secret that you can upload to your GitHub repository. There are thousands of Actions (that are free) for a wide variety of tasks that can be discovered on the GitHub Marketplace. The ability to inherit ready-made Actions in your workflow allows you to accomplish complex tasks without implementing all of the logic yourself. Some useful Actions for those getting started are: . actions/checkout: Allows you to quickly clone the contents of your repository into your environment, which you often want to do. This does a number of other things such as automatically mount your repository’s files into downstream Docker containers. | mxschmitt/action-tmate: Proivdes a way to debug Actions interactively. This uses port forwarding to give you a terminal in the browser that is connected to your Actions runner. Be careful not to expose sensitive information if you use this. | actions/github-script: Gives you a pre-authenticated ocotokit.js client that allows you to interact with the GitHub API to accomplish almost any task on GitHub automatically. Only these endpoints are supported (for example, the secrets endpoint is not in that list). | . In addition to the aforementioned Actions, it is helpful to go peruse the official GitHub Actions docs before diving in. . Example: A fastpages Action Workflow . The best to way familiarize yourself with Actions is by studying examples. Let’s take a look at the Action workflow that automates the build of fastpages (the platform used to write this blog post). . Part 1: Define Workflow Triggers . blog, defined in ci.yaml. Like all Actions workflows, this is YAML file is located in the .github/workflows directory of the GitHub repo. . The top of this YAML file looks like this: . name: CI on: push: branches: - master pull_request: . This means that this workflow is triggered on either a push or pull request event. Furthermore, push events are filtered such that only pushes to the master branch will trigger the workflow, whereas all pull requests will trigger this workflow. It is important to note that pull requests opened from forks will have read-only access to the base repository and cannot access any secrets for security reasons. The reason for defining the workflow in this way is we wanted to trigger the same workflow to test pull requests as well as build and deploy the website when a PR is merged into master. This will be clarified as we step through the rest of the YAML file. . Part 2: Define Jobs . Next, we define jobs (there is only one in this workflow). Per the docs: . A workflow run is made up of one or more jobs. Jobs run in parallel by default. . jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: . The keyword build-site is the name of your job and you can name it whatever you want. In this case, we have a conditional if statement that dictates if this job should be run or not. We are trying to ensure that this workflow does not run when the first commit to a repo is made with the message ‘Initial commit’. The first variable in the if statement, github.event, contains a json payload of the event that triggered this workflow. When developing workflows, it is helpful to print this variable in order to inspect its structure, which you can accomplish with the following YAML: . - name: see payload run: | echo &quot;PAYLOAD: n${PAYLOAD} n&quot; env: PAYLOAD: ${{ toJSON(github.event) }} . Note: the above step is only for debugging and is not currently in the workflow. . toJson is a handy function that returns a pretty-printed JSON representation of the variable. The output is printed directly in the logs contained in the Actions tab of your repo. In this example, printing the payload for a push event will look like this (truncated for brevity): . { &quot;ref&quot;: &quot;refs/tags/simple-tag&quot;, &quot;before&quot;: &quot;6113728f27ae8c7b1a77c8d03f9ed6e0adf246&quot;, &quot;created&quot;: false, &quot;deleted&quot;: true, &quot;forced&quot;: false, &quot;base_ref&quot;: null, &quot;commits&quot;: [ { &quot;message&quot;: &quot;updated README.md&quot;, &quot;author&quot;: &quot;hamelsmu&quot; }, ], &quot;head_commit&quot;: null, } . Therefore, the variable github.event.commits[0].message will retrieve the first commit message in the array of commits. Since we are looking for situations where there is only one commit, this logic suffices. The second variable in the if statement, github.run_number is a special variable in Actions which: . [is a] unique number for each run of a particular workflow in a repository. This number begins at 1 for the workflow’s first run, and increments with each new run. This number does not change if you re-run the workflow run. . Therefore, the if statement introduced above: . if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 . Allows the workflow to run when the commit message is “Initial commit” as long as it is not the first commit. ( || is a logical or operator). . Finally, the line runs-on: ubuntu-latest specifies the host operating system that your workflows will run in. . Part 3: Define Steps . Per the docs: . A job contains a sequence of tasks called steps. Steps can run commands, run setup tasks, or run an Action in your repository, a public repository, or an Action published in a Docker registry. Not all steps run Actions, but all Actions run as a step. Each step runs in its own process in the runner environment and has access to the workspace and filesystem. Because steps run in their own process, changes to environment variables are not preserved between steps. GitHub provides built-in steps to set up and complete a job. . Below are the first two steps in our workflow: . - name: Copy Repository Contents uses: actions/checkout@master with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files . The first step creates a copy of your repository in the Actions file system, with the help of the utility action/checkout. This utility only fetches the last commit by default and saves files into a directory (whose path is stored in the environment variable GITHUB_WORKSPACE that is accessible by subsequent steps in your job. The second step runs the fastai/fastpages Action, which converts notebooks and word documents to blog posts automatically. In this case, the syntax: . uses: ./_action_files . is a special case where the pre-made GitHub Action we want to run happens to be defined in the same repo that runs this workflow. This syntax allows us to test changes to this pre-made Action when evaluating PRs by referencing the directory in the current repository that defines that pre-made Action. Note: Building pre-made Actions is beyond the scope of this tutorial. . The next three steps in our workflow are defined below: . - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://hamelsmu/fastpages-jekyll with: args: bash -c &quot;gem install bundler &amp;&amp; jekyll build -V&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : . The step named setup directories for Jekyll build executes shell commands that remove the _site folder in order to get rid of stale files related to the page we want to build, as well as grant permissions to all the files in our repo to subsequent steps. . The step named Jekyll build executes a docker container hosted by the Jekyll community on Dockerhub called jekyll/jekyll. For those not familiar with Docker, see this tutorial. The name of this container is called hamelsmu/fastpages-jekyll because I’m adding some additional dependencies to jekyll/jekyll and hosting those on my DockerHub account for faster build times2. The args parameter allows you to execute arbitrary commands with the Docker container by overriding the CMD instruction in the Dockerfile. We use this Docker container hosted on Dockerhub so we don’t have to deal with installing and configuring all of the complicated dependencies for Jekyll. The files from our repo are already available in the Actions runtime due to the first step in this workflow, and are mounted into this Docker container automatically for us. In this case, we are running the command jekyll build, which builds our website and places relevant assets them into the _site folder. For more information about Jekyll, read the official docs. Finally, the env parameter allows me to pass an environment variable into the Docker container. . The final command above copies a CNAME file into the _site folder, which we need for the custom domain https://fastpages.fast.ai. Setting up custom domains are outside the scope of this article. . The final step in our workflow is defined below: . - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.SSH_DEPLOY_KEY }} publish_dir: ./_site . The statement . if: github.event_name == &#39;push&#39; . uses the variable github.event_name to ensure this step only runs when a push event ( in this case only pushes to the master branch trigger this workflow) occur. . This step deploys the fastpages website by copying the contents of the _site folder to the root of the gh-pages branch, which GitHub Pages uses for hosting. This step uses the peaceiris/actions-gh-pages Action, pinned at version 3. Their README describes various options and inputs for this Action. . Conclusion . We hope that this has shed some light on how we use GitHub Actions to automate fastpages. While we only covered one workflow above, we hope this provides enough intuition to understand the other workflows in fastpages. We have only scratched the surface of GitHub Actions in this blog post, but we provide other materials below for those who want to dive in deeper. We have not covered how to host an Action for other people, but you can start with these docs to learn more. . Still confused about how GitHub Actions could be used for Data Science? Here are some ideas of things you can build: . Jupyter Widgets that trigger GitHub Actions to perform various tasks on GitHub via the repository dispatch event | Integration with Pachyderm for data versioning. | Integration with your favorite cloud machine learning services, such Sagemaker, Azure ML or GCP’s AI Platform. | . Related Materials . GitHub Actions official documentation | Hello world Docker Action: A template to demonstrate how to build a Docker Action for other people to use. | Awesome Actions: A curated list of interesting GitHub Actions by topic. | A tutorial on Docker for Data Scientists. | . Getting In Touch . Please feel free to get in touch with us on Twitter: . Hamel Husain @HamelHusain | Jeremy Howard @jeremyphoward | . . Footnotes . You can see some of Hugging Face’s Actions workflows for machine learning on GitHub &#8617; . | These additional dependencies are defined here, which uses the “jekyll build” command to add ruby dedpendencies from the Gemfile located at the root of the repo. Additionally, this docker image is built by another Action workflow defined here. &#8617; . |",
            "url": "/actions/markdown/2020/03/06/fastpages-actions.html",
            "relUrl": "/actions/markdown/2020/03/06/fastpages-actions.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides has for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Feautures . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Footnotes . This is the footnote. &#8617; . |",
            "url": "/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "关于我",
          "content": "这是 About（关于） 页面。 像其他页面一样, 这是用Markdown格式书写的。 . 此网站由 fastpages 1提供技术支持。 . 此网站由 EasonQYS 2提供汉化支持。 . 一个除了普通格式外还支持Jupyter的博客。 &#8617; . | 第一个中文汉化版本。 &#8617; . |",
          "url": "/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}